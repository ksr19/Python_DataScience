{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регуляризация является одним из методов борьбы с переобучением в моделях. Обычно переобучение выражается в слишком больших значениях параметров, которые позволяют достичь очень хороших результатов на тренировочной выборке, а результаты на тесте получаются значительно хуже. Бороться с этой проблемой можно добавив штрафы для параметров в функцию потерь, которую мы минимизируем. Чаще всего используются L1 и L2 регуляризации, которые различаются применяемыми нормами. В случае решающих деревьев регуляризация обычно выражается в форме ограничения количества листьев при разделении, ограничении глубины дерева, определении минимального количества наблюдений, которое должен покрывать каждый лист дерева. Кроме того может задаваться определенный уровень уменьшения неопределенности, который нужно превысить, чтобы произвести разделение узла дерева. Также построение ансамблей деревьев с ограничением максимального количества деревьев может помочь в минимизации переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. По какому принципу рассчитывается \"важность признака (feature_importance)\" в ансамблях деревьев?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процедура расчета важности признаков в ансамблях деревьев состоит из нескольких этапов. Изначально для каждого признака в каждом построенном дереве рассчитывается взвешенное уменьшение неопределенности, достигаемое в результате разделения по данному признаку. \n",
    "Допустим,  имеется некий узел, который на основе признака $i$  разделяется на некоторое количество листьев. Для каждого из листьев необходимо посчитать так называемую неопределенность Джини (Gini’s impurity), которая вычисляется по формуле $G=\\sum p(j)\\cdot(1−p(j))$. Суммирование проходит по всем классам: $j = 1, …, K$; $p(j)$- вероятность случайно выбрать элемент, принадлежащий классу $j$, из наблюдений достигших данного листа. Значение данного показателя равное нулю указывает на полное отсутствие неопределенности. Для самого узла этот показатель должен быть посчитан ранее (на предыдущем шаге) и к моменту разделения он уже известен. Далее используя весовые коэффициенты для узла и листьев, а именно отношение количества наблюдений, достигших этого узла/листа, к общему количеству наблюдений, можно рассчитать важность данного узла, а точнее разделения данного узла на основе признака $i$.  Для этого необходимо из взвешенной неопределенности Джини для узла вычесть сумму взвешенных неопределенностей для каждого листа. Теперь необходимо произвести аналогичные вычисления для всех узлов, где происходит разделение по признаку $i$. Затем аналогичную процедуру повторяем для каждого анализируемого признака. Отношение суммы важностей всех узлов, в которых произошло разделение по признаку $i$ , к общей сумме важности всех узлов и есть показателем важности признака в данном дереве.  Далее рассчитывается среднее значение этой величины для всех деревьев. Получившееся значение и есть окончательным показателем важности признака $i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
